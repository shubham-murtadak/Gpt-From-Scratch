{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os       # os.path.exists\n",
        "import math     # math.log, math.exp\n",
        "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
        "random.seed(42) # Let there be order among chaos\n",
        "\n",
        "# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of logs)\n",
        "DATA_PATH = '/content/logs_5000.txt'\n",
        "\n",
        "docs = [l.strip() for l in open(DATA_PATH).read().split('\\n') if l.strip()]\n",
        "random.shuffle(docs)\n",
        "print(f\"num docs: {len(docs)}\")"
      ],
      "metadata": {
        "id": "arsgkcFPzhqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5c6131-9574-4727-f45c-d08df2d826e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num docs: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be a Tokenizer to translate strings to discrete symbols and back\n",
        "uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\n",
        "BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token\n",
        "vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\n",
        "print(f\"vocab size: {vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esvw1blr7LLj",
        "outputId": "175994a9-8a37-4d1e-8807-04d79c4c7844"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let there be Autograd, to recursively apply the chain rule through a computation graph\n",
        "class Value:\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # scalar value of this node calculated during forward pass\n",
        "        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n",
        "        self._children = children       # children of this node in the computation graph\n",
        "        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad"
      ],
      "metadata": {
        "id": "QkwDc7JA7Tzo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the parameters, to store the knowledge of the model.\n",
        "n_embd = 24     # embedding dimension\n",
        "n_head = 4      # number of attention heads\n",
        "n_layer = 1     # number of layers\n",
        "block_size = 32 # maximum sequence length\n",
        "head_dim = n_embd // n_head # dimension of each head\n",
        "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\n",
        "print(f\"num params: {len(params)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuo2Hltc7dJZ",
        "outputId": "b270558d-fd72-4a42-a111-5f28d407437c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num params: 9696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.\n",
        "# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU\n",
        "def linear(x, w):\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "def softmax(logits):\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "def gpt(token_id, pos_id, keys, values):\n",
        "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
        "    pos_emb = state_dict['wpe'][pos_id] # position embedding\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n",
        "    x = rmsnorm(x)\n",
        "\n",
        "    for li in range(n_layer):\n",
        "        # 1) Multi-head attention block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs+head_dim]\n",
        "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
        "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
        "            attn_weights = softmax(attn_logits)\n",
        "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
        "            x_attn.extend(head_out)\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "        # 2) MLP block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "    logits = linear(x, state_dict['lm_head'])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "bYAk6lN57lcC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be Adam, the blessed optimizer and its buffers\n",
        "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
        "m = [0.0] * len(params) # first moment buffer\n",
        "v = [0.0] * len(params) # second moment buffer\n"
      ],
      "metadata": {
        "id": "nFKvsM7P7xZQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat in sequence\n",
        "num_steps = 1000 # number of training steps\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # Take single document, tokenize it, surround it with BOS special token on both sides\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
        "    n = min(block_size, len(tokens) - 1)\n",
        "\n",
        "    # Forward the token sequence through the model, building up the computation graph all the way to the loss.\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        loss_t = -probs[target_id].log()\n",
        "        losses.append(loss_t)\n",
        "    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n",
        "\n",
        "    # Backward the loss, calculating the gradients with respect to all model parameters.\n",
        "    loss.backward()\n",
        "\n",
        "    # Adam optimizer update: update the model parameters based on the corresponding gradients.\n",
        "    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
        "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
        "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
        "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
        "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "\n",
        "    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIWJrwuY74GT",
        "outputId": "babf7efc-3345-472d-f9d8-6889d1865f99"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    1 / 1000 | loss 2.2478\n",
            "step    2 / 1000 | loss 2.2138\n",
            "step    3 / 1000 | loss 2.6572\n",
            "step    4 / 1000 | loss 2.9876\n",
            "step    5 / 1000 | loss 2.6668\n",
            "step    6 / 1000 | loss 2.6081\n",
            "step    7 / 1000 | loss 3.0399\n",
            "step    8 / 1000 | loss 2.6982\n",
            "step    9 / 1000 | loss 3.4015\n",
            "step   10 / 1000 | loss 2.9190\n",
            "step   11 / 1000 | loss 2.4156\n",
            "step   12 / 1000 | loss 2.9301\n",
            "step   13 / 1000 | loss 3.1057\n",
            "step   14 / 1000 | loss 2.4994\n",
            "step   15 / 1000 | loss 3.3957\n",
            "step   16 / 1000 | loss 2.2030\n",
            "step   17 / 1000 | loss 2.7041\n",
            "step   18 / 1000 | loss 2.6138\n",
            "step   19 / 1000 | loss 3.1638\n",
            "step   20 / 1000 | loss 2.7406\n",
            "step   21 / 1000 | loss 2.0568\n",
            "step   22 / 1000 | loss 3.1725\n",
            "step   23 / 1000 | loss 2.8709\n",
            "step   24 / 1000 | loss 3.0415\n",
            "step   25 / 1000 | loss 3.1318\n",
            "step   26 / 1000 | loss 2.9678\n",
            "step   27 / 1000 | loss 3.0054\n",
            "step   28 / 1000 | loss 3.1860\n",
            "step   29 / 1000 | loss 2.7888\n",
            "step   30 / 1000 | loss 2.6011\n",
            "step   31 / 1000 | loss 2.5457\n",
            "step   32 / 1000 | loss 2.5364\n",
            "step   33 / 1000 | loss 2.7660\n",
            "step   34 / 1000 | loss 2.6292\n",
            "step   35 / 1000 | loss 2.9098\n",
            "step   36 / 1000 | loss 3.0470\n",
            "step   37 / 1000 | loss 2.4995\n",
            "step   38 / 1000 | loss 2.6711\n",
            "step   39 / 1000 | loss 2.4649\n",
            "step   40 / 1000 | loss 2.6996\n",
            "step   41 / 1000 | loss 2.6991\n",
            "step   42 / 1000 | loss 2.8246\n",
            "step   43 / 1000 | loss 2.7654\n",
            "step   44 / 1000 | loss 2.7895\n",
            "step   45 / 1000 | loss 2.2775\n",
            "step   46 / 1000 | loss 2.4295\n",
            "step   47 / 1000 | loss 2.5884\n",
            "step   48 / 1000 | loss 2.6007\n",
            "step   49 / 1000 | loss 2.4115\n",
            "step   50 / 1000 | loss 2.4767\n",
            "step   51 / 1000 | loss 2.3458\n",
            "step   52 / 1000 | loss 2.5962\n",
            "step   53 / 1000 | loss 2.7230\n",
            "step   54 / 1000 | loss 2.8292\n",
            "step   55 / 1000 | loss 2.4541\n",
            "step   56 / 1000 | loss 3.0183\n",
            "step   57 / 1000 | loss 2.1680\n",
            "step   58 / 1000 | loss 2.4215\n",
            "step   59 / 1000 | loss 2.6569\n",
            "step   60 / 1000 | loss 2.1563\n",
            "step   61 / 1000 | loss 2.2790\n",
            "step   62 / 1000 | loss 2.7422\n",
            "step   63 / 1000 | loss 2.8078\n",
            "step   64 / 1000 | loss 2.5933\n",
            "step   65 / 1000 | loss 2.9040\n",
            "step   66 / 1000 | loss 2.3493\n",
            "step   67 / 1000 | loss 2.3636\n",
            "step   68 / 1000 | loss 2.1852\n",
            "step   69 / 1000 | loss 2.3803\n",
            "step   70 / 1000 | loss 2.3505\n",
            "step   71 / 1000 | loss 2.6459\n",
            "step   72 / 1000 | loss 1.8410\n",
            "step   73 / 1000 | loss 2.6529\n",
            "step   74 / 1000 | loss 2.5477\n",
            "step   75 / 1000 | loss 2.6215\n",
            "step   76 / 1000 | loss 2.4805\n",
            "step   77 / 1000 | loss 2.3266\n",
            "step   78 / 1000 | loss 2.2944\n",
            "step   79 / 1000 | loss 1.9916\n",
            "step   80 / 1000 | loss 2.5132\n",
            "step   81 / 1000 | loss 2.1922\n",
            "step   82 / 1000 | loss 2.5888\n",
            "step   83 / 1000 | loss 3.0137\n",
            "step   84 / 1000 | loss 2.2786\n",
            "step   85 / 1000 | loss 2.6372\n",
            "step   86 / 1000 | loss 2.8176\n",
            "step   87 / 1000 | loss 2.4914\n",
            "step   88 / 1000 | loss 2.5495\n",
            "step   89 / 1000 | loss 2.4817\n",
            "step   90 / 1000 | loss 1.9610\n",
            "step   91 / 1000 | loss 2.8429\n",
            "step   92 / 1000 | loss 2.3786\n",
            "step   93 / 1000 | loss 2.5553\n",
            "step   94 / 1000 | loss 2.8788\n",
            "step   95 / 1000 | loss 2.0370\n",
            "step   96 / 1000 | loss 2.1189\n",
            "step   97 / 1000 | loss 2.4795\n",
            "step   98 / 1000 | loss 2.6252\n",
            "step   99 / 1000 | loss 2.4375\n",
            "step  100 / 1000 | loss 2.6437\n",
            "step  101 / 1000 | loss 2.4609\n",
            "step  102 / 1000 | loss 2.3826\n",
            "step  103 / 1000 | loss 2.5118\n",
            "step  104 / 1000 | loss 2.7559\n",
            "step  105 / 1000 | loss 2.7772\n",
            "step  106 / 1000 | loss 2.0789\n",
            "step  107 / 1000 | loss 2.1235\n",
            "step  108 / 1000 | loss 2.4383\n",
            "step  109 / 1000 | loss 3.1274\n",
            "step  110 / 1000 | loss 2.9098\n",
            "step  111 / 1000 | loss 2.4791\n",
            "step  112 / 1000 | loss 2.3200\n",
            "step  113 / 1000 | loss 2.6098\n",
            "step  114 / 1000 | loss 2.3955\n",
            "step  115 / 1000 | loss 2.2166\n",
            "step  116 / 1000 | loss 2.5205\n",
            "step  117 / 1000 | loss 2.4514\n",
            "step  118 / 1000 | loss 2.3950\n",
            "step  119 / 1000 | loss 2.3933\n",
            "step  120 / 1000 | loss 2.2287\n",
            "step  121 / 1000 | loss 2.6326\n",
            "step  122 / 1000 | loss 2.6948\n",
            "step  123 / 1000 | loss 2.2496\n",
            "step  124 / 1000 | loss 2.3941\n",
            "step  125 / 1000 | loss 2.3740\n",
            "step  126 / 1000 | loss 2.2308\n",
            "step  127 / 1000 | loss 2.5997\n",
            "step  128 / 1000 | loss 2.2938\n",
            "step  129 / 1000 | loss 2.9174\n",
            "step  130 / 1000 | loss 2.6234\n",
            "step  131 / 1000 | loss 2.2863\n",
            "step  132 / 1000 | loss 2.1927\n",
            "step  133 / 1000 | loss 2.3149\n",
            "step  134 / 1000 | loss 2.8279\n",
            "step  135 / 1000 | loss 2.6701\n",
            "step  136 / 1000 | loss 2.5288\n",
            "step  137 / 1000 | loss 2.2430\n",
            "step  138 / 1000 | loss 2.0271\n",
            "step  139 / 1000 | loss 2.3200\n",
            "step  140 / 1000 | loss 2.5314\n",
            "step  141 / 1000 | loss 2.4075\n",
            "step  142 / 1000 | loss 2.3778\n",
            "step  143 / 1000 | loss 2.3361\n",
            "step  144 / 1000 | loss 2.3277\n",
            "step  145 / 1000 | loss 2.5499\n",
            "step  146 / 1000 | loss 2.3735\n",
            "step  147 / 1000 | loss 2.3986\n",
            "step  148 / 1000 | loss 2.4358\n",
            "step  149 / 1000 | loss 2.1529\n",
            "step  150 / 1000 | loss 1.9831\n",
            "step  151 / 1000 | loss 2.2736\n",
            "step  152 / 1000 | loss 2.3464\n",
            "step  153 / 1000 | loss 2.5502\n",
            "step  154 / 1000 | loss 2.2726\n",
            "step  155 / 1000 | loss 2.3385\n",
            "step  156 / 1000 | loss 2.3029\n",
            "step  157 / 1000 | loss 2.0711\n",
            "step  158 / 1000 | loss 2.1946\n",
            "step  159 / 1000 | loss 2.2960\n",
            "step  160 / 1000 | loss 2.0231\n",
            "step  161 / 1000 | loss 2.1796\n",
            "step  162 / 1000 | loss 2.8704\n",
            "step  163 / 1000 | loss 2.1452\n",
            "step  164 / 1000 | loss 1.9413\n",
            "step  165 / 1000 | loss 2.3831\n",
            "step  166 / 1000 | loss 2.5070\n",
            "step  167 / 1000 | loss 2.0711\n",
            "step  168 / 1000 | loss 2.1667\n",
            "step  169 / 1000 | loss 1.8743\n",
            "step  170 / 1000 | loss 2.6175\n",
            "step  171 / 1000 | loss 2.2264\n",
            "step  172 / 1000 | loss 2.8442\n",
            "step  173 / 1000 | loss 2.7444\n",
            "step  174 / 1000 | loss 2.3022\n",
            "step  175 / 1000 | loss 1.9877\n",
            "step  176 / 1000 | loss 2.8451\n",
            "step  177 / 1000 | loss 2.2401\n",
            "step  178 / 1000 | loss 2.0065\n",
            "step  179 / 1000 | loss 2.4871\n",
            "step  180 / 1000 | loss 2.4841\n",
            "step  181 / 1000 | loss 2.2526\n",
            "step  182 / 1000 | loss 2.4048\n",
            "step  183 / 1000 | loss 2.4730\n",
            "step  184 / 1000 | loss 2.1524\n",
            "step  185 / 1000 | loss 2.7802\n",
            "step  186 / 1000 | loss 2.3349\n",
            "step  187 / 1000 | loss 2.2508\n",
            "step  188 / 1000 | loss 2.5119\n",
            "step  189 / 1000 | loss 2.3511\n",
            "step  190 / 1000 | loss 2.3190\n",
            "step  191 / 1000 | loss 2.3814\n",
            "step  192 / 1000 | loss 2.3702\n",
            "step  193 / 1000 | loss 2.4604\n",
            "step  194 / 1000 | loss 2.3132\n",
            "step  195 / 1000 | loss 2.2993\n",
            "step  196 / 1000 | loss 2.6737\n",
            "step  197 / 1000 | loss 1.9155\n",
            "step  198 / 1000 | loss 2.1572\n",
            "step  199 / 1000 | loss 2.3513\n",
            "step  200 / 1000 | loss 2.1149\n",
            "step  201 / 1000 | loss 2.4215\n",
            "step  202 / 1000 | loss 2.2224\n",
            "step  203 / 1000 | loss 1.7263\n",
            "step  204 / 1000 | loss 2.1626\n",
            "step  205 / 1000 | loss 2.6242\n",
            "step  206 / 1000 | loss 2.2919\n",
            "step  207 / 1000 | loss 2.0550\n",
            "step  208 / 1000 | loss 2.6099\n",
            "step  209 / 1000 | loss 1.9967\n",
            "step  210 / 1000 | loss 2.1996\n",
            "step  211 / 1000 | loss 2.3208\n",
            "step  212 / 1000 | loss 2.2935\n",
            "step  213 / 1000 | loss 2.1665\n",
            "step  214 / 1000 | loss 2.5620\n",
            "step  215 / 1000 | loss 2.2156\n",
            "step  216 / 1000 | loss 2.1263\n",
            "step  217 / 1000 | loss 1.9304\n",
            "step  218 / 1000 | loss 2.1381\n",
            "step  219 / 1000 | loss 2.8838\n",
            "step  220 / 1000 | loss 2.2745\n",
            "step  221 / 1000 | loss 2.1607\n",
            "step  222 / 1000 | loss 2.2334\n",
            "step  223 / 1000 | loss 2.0176\n",
            "step  224 / 1000 | loss 2.3046\n",
            "step  225 / 1000 | loss 2.3145\n",
            "step  226 / 1000 | loss 2.2452\n",
            "step  227 / 1000 | loss 2.8454\n",
            "step  228 / 1000 | loss 2.3490\n",
            "step  229 / 1000 | loss 2.9037\n",
            "step  230 / 1000 | loss 2.4867\n",
            "step  231 / 1000 | loss 1.8711\n",
            "step  232 / 1000 | loss 2.1267\n",
            "step  233 / 1000 | loss 2.6441\n",
            "step  234 / 1000 | loss 2.7755\n",
            "step  235 / 1000 | loss 2.7520\n",
            "step  236 / 1000 | loss 2.5684\n",
            "step  237 / 1000 | loss 2.6738\n",
            "step  238 / 1000 | loss 2.4499\n",
            "step  239 / 1000 | loss 2.1953\n",
            "step  240 / 1000 | loss 2.3517\n",
            "step  241 / 1000 | loss 1.8194\n",
            "step  242 / 1000 | loss 2.3500\n",
            "step  243 / 1000 | loss 2.0511\n",
            "step  244 / 1000 | loss 2.6622\n",
            "step  245 / 1000 | loss 2.0903\n",
            "step  246 / 1000 | loss 2.5277\n",
            "step  247 / 1000 | loss 2.2408\n",
            "step  248 / 1000 | loss 2.6500\n",
            "step  249 / 1000 | loss 2.4013\n",
            "step  250 / 1000 | loss 2.3552\n",
            "step  251 / 1000 | loss 2.2248\n",
            "step  252 / 1000 | loss 2.8089\n",
            "step  253 / 1000 | loss 2.3436\n",
            "step  254 / 1000 | loss 2.1312\n",
            "step  255 / 1000 | loss 2.2986\n",
            "step  256 / 1000 | loss 2.1254\n",
            "step  257 / 1000 | loss 2.0386\n",
            "step  258 / 1000 | loss 2.6087\n",
            "step  259 / 1000 | loss 2.0902\n",
            "step  260 / 1000 | loss 1.6495\n",
            "step  261 / 1000 | loss 2.3828\n",
            "step  262 / 1000 | loss 1.8846\n",
            "step  263 / 1000 | loss 2.4934\n",
            "step  264 / 1000 | loss 2.2396\n",
            "step  265 / 1000 | loss 2.6915\n",
            "step  266 / 1000 | loss 2.2943\n",
            "step  267 / 1000 | loss 2.2681\n",
            "step  268 / 1000 | loss 2.3846\n",
            "step  269 / 1000 | loss 2.2128\n",
            "step  270 / 1000 | loss 2.2729\n",
            "step  271 / 1000 | loss 2.0614\n",
            "step  272 / 1000 | loss 2.1913\n",
            "step  273 / 1000 | loss 2.0809\n",
            "step  274 / 1000 | loss 2.2363\n",
            "step  275 / 1000 | loss 2.6635\n",
            "step  276 / 1000 | loss 2.2013\n",
            "step  277 / 1000 | loss 2.0511\n",
            "step  278 / 1000 | loss 2.1509\n",
            "step  279 / 1000 | loss 1.9030\n",
            "step  280 / 1000 | loss 2.0671\n",
            "step  281 / 1000 | loss 2.1556\n",
            "step  282 / 1000 | loss 2.1053\n",
            "step  283 / 1000 | loss 2.1029\n",
            "step  284 / 1000 | loss 2.4451\n",
            "step  285 / 1000 | loss 2.3300\n",
            "step  286 / 1000 | loss 1.8325\n",
            "step  287 / 1000 | loss 2.2393\n",
            "step  288 / 1000 | loss 2.3979\n",
            "step  289 / 1000 | loss 1.9864\n",
            "step  290 / 1000 | loss 2.2534\n",
            "step  291 / 1000 | loss 1.6515\n",
            "step  292 / 1000 | loss 1.6192\n",
            "step  293 / 1000 | loss 2.5937\n",
            "step  294 / 1000 | loss 1.8961\n",
            "step  295 / 1000 | loss 2.6114\n",
            "step  296 / 1000 | loss 2.2686\n",
            "step  297 / 1000 | loss 1.5053\n",
            "step  298 / 1000 | loss 2.1525\n",
            "step  299 / 1000 | loss 2.1685\n",
            "step  300 / 1000 | loss 2.2929\n",
            "step  301 / 1000 | loss 2.2319\n",
            "step  302 / 1000 | loss 2.1718\n",
            "step  303 / 1000 | loss 2.2161\n",
            "step  304 / 1000 | loss 1.5979\n",
            "step  305 / 1000 | loss 2.6249\n",
            "step  306 / 1000 | loss 1.6391\n",
            "step  307 / 1000 | loss 2.2739\n",
            "step  308 / 1000 | loss 1.8777\n",
            "step  309 / 1000 | loss 2.4067\n",
            "step  310 / 1000 | loss 1.7627\n",
            "step  311 / 1000 | loss 2.0698\n",
            "step  312 / 1000 | loss 2.7719\n",
            "step  313 / 1000 | loss 2.1069\n",
            "step  314 / 1000 | loss 2.0020\n",
            "step  315 / 1000 | loss 2.4244\n",
            "step  316 / 1000 | loss 1.6847\n",
            "step  317 / 1000 | loss 2.2276\n",
            "step  318 / 1000 | loss 1.9201\n",
            "step  319 / 1000 | loss 1.8710\n",
            "step  320 / 1000 | loss 1.9546\n",
            "step  321 / 1000 | loss 2.0223\n",
            "step  322 / 1000 | loss 2.4581\n",
            "step  323 / 1000 | loss 2.3601\n",
            "step  324 / 1000 | loss 2.3106\n",
            "step  325 / 1000 | loss 2.3399\n",
            "step  326 / 1000 | loss 1.8398\n",
            "step  327 / 1000 | loss 1.8898\n",
            "step  328 / 1000 | loss 1.9075\n",
            "step  329 / 1000 | loss 2.0636\n",
            "step  330 / 1000 | loss 1.9863\n",
            "step  331 / 1000 | loss 1.7974\n",
            "step  332 / 1000 | loss 2.2357\n",
            "step  333 / 1000 | loss 2.3946\n",
            "step  334 / 1000 | loss 2.0815\n",
            "step  335 / 1000 | loss 2.4109\n",
            "step  336 / 1000 | loss 1.8079\n",
            "step  337 / 1000 | loss 2.6387\n",
            "step  338 / 1000 | loss 2.1071\n",
            "step  339 / 1000 | loss 2.3307\n",
            "step  340 / 1000 | loss 2.2827\n",
            "step  341 / 1000 | loss 2.2443\n",
            "step  342 / 1000 | loss 2.0203\n",
            "step  343 / 1000 | loss 2.2924\n",
            "step  344 / 1000 | loss 2.1491\n",
            "step  345 / 1000 | loss 1.8286\n",
            "step  346 / 1000 | loss 1.9752\n",
            "step  347 / 1000 | loss 2.0102\n",
            "step  348 / 1000 | loss 2.4113\n",
            "step  349 / 1000 | loss 2.0457\n",
            "step  350 / 1000 | loss 1.9692\n",
            "step  351 / 1000 | loss 2.4649\n",
            "step  352 / 1000 | loss 2.0312\n",
            "step  353 / 1000 | loss 1.7503\n",
            "step  354 / 1000 | loss 1.7134\n",
            "step  355 / 1000 | loss 2.2157\n",
            "step  356 / 1000 | loss 2.0833\n",
            "step  357 / 1000 | loss 2.2514\n",
            "step  358 / 1000 | loss 2.0859\n",
            "step  359 / 1000 | loss 2.2815\n",
            "step  360 / 1000 | loss 1.8179\n",
            "step  361 / 1000 | loss 2.6980\n",
            "step  362 / 1000 | loss 2.0478\n",
            "step  363 / 1000 | loss 2.3780\n",
            "step  364 / 1000 | loss 2.4549\n",
            "step  365 / 1000 | loss 1.8775\n",
            "step  366 / 1000 | loss 2.0360\n",
            "step  367 / 1000 | loss 1.6102\n",
            "step  368 / 1000 | loss 1.8267\n",
            "step  369 / 1000 | loss 2.0791\n",
            "step  370 / 1000 | loss 2.4086\n",
            "step  371 / 1000 | loss 2.2682\n",
            "step  372 / 1000 | loss 1.9456\n",
            "step  373 / 1000 | loss 1.5512\n",
            "step  374 / 1000 | loss 2.0893\n",
            "step  375 / 1000 | loss 1.9951\n",
            "step  376 / 1000 | loss 1.7401\n",
            "step  377 / 1000 | loss 1.4567\n",
            "step  378 / 1000 | loss 1.5587\n",
            "step  379 / 1000 | loss 1.8379\n",
            "step  380 / 1000 | loss 3.0023\n",
            "step  381 / 1000 | loss 2.2302\n",
            "step  382 / 1000 | loss 1.2997\n",
            "step  383 / 1000 | loss 1.5176\n",
            "step  384 / 1000 | loss 1.9304\n",
            "step  385 / 1000 | loss 2.3632\n",
            "step  386 / 1000 | loss 1.7989\n",
            "step  387 / 1000 | loss 1.7446\n",
            "step  388 / 1000 | loss 1.7671\n",
            "step  389 / 1000 | loss 1.8026\n",
            "step  390 / 1000 | loss 1.8420\n",
            "step  391 / 1000 | loss 2.2925\n",
            "step  392 / 1000 | loss 1.9985\n",
            "step  393 / 1000 | loss 2.5852\n",
            "step  394 / 1000 | loss 1.7746\n",
            "step  395 / 1000 | loss 1.7355\n",
            "step  396 / 1000 | loss 1.7376\n",
            "step  397 / 1000 | loss 2.6695\n",
            "step  398 / 1000 | loss 2.2355\n",
            "step  399 / 1000 | loss 2.0353\n",
            "step  400 / 1000 | loss 2.0389\n",
            "step  401 / 1000 | loss 1.4517\n",
            "step  402 / 1000 | loss 2.4054\n",
            "step  403 / 1000 | loss 1.5644\n",
            "step  404 / 1000 | loss 2.7886\n",
            "step  405 / 1000 | loss 1.8227\n",
            "step  406 / 1000 | loss 2.0216\n",
            "step  407 / 1000 | loss 2.2881\n",
            "step  408 / 1000 | loss 2.2274\n",
            "step  409 / 1000 | loss 1.7309\n",
            "step  410 / 1000 | loss 1.7312\n",
            "step  411 / 1000 | loss 1.8189\n",
            "step  412 / 1000 | loss 2.2611\n",
            "step  413 / 1000 | loss 1.6865\n",
            "step  414 / 1000 | loss 2.2053\n",
            "step  415 / 1000 | loss 2.2915\n",
            "step  416 / 1000 | loss 2.3180\n",
            "step  417 / 1000 | loss 2.1774\n",
            "step  418 / 1000 | loss 1.9666\n",
            "step  419 / 1000 | loss 1.3566\n",
            "step  420 / 1000 | loss 2.1953\n",
            "step  421 / 1000 | loss 1.5378\n",
            "step  422 / 1000 | loss 1.4961\n",
            "step  423 / 1000 | loss 2.6122\n",
            "step  424 / 1000 | loss 2.3892\n",
            "step  425 / 1000 | loss 2.2845\n",
            "step  426 / 1000 | loss 2.2756\n",
            "step  427 / 1000 | loss 2.2435\n",
            "step  428 / 1000 | loss 2.1405\n",
            "step  429 / 1000 | loss 2.1607\n",
            "step  430 / 1000 | loss 1.6427\n",
            "step  431 / 1000 | loss 2.1085\n",
            "step  432 / 1000 | loss 2.0271\n",
            "step  433 / 1000 | loss 2.4358\n",
            "step  434 / 1000 | loss 1.6990\n",
            "step  435 / 1000 | loss 2.4269\n",
            "step  436 / 1000 | loss 1.9274\n",
            "step  437 / 1000 | loss 1.9055\n",
            "step  438 / 1000 | loss 1.8232\n",
            "step  439 / 1000 | loss 1.4214\n",
            "step  440 / 1000 | loss 2.5194\n",
            "step  441 / 1000 | loss 2.0751\n",
            "step  442 / 1000 | loss 2.4940\n",
            "step  443 / 1000 | loss 2.4402\n",
            "step  444 / 1000 | loss 2.5789\n",
            "step  445 / 1000 | loss 2.7473\n",
            "step  446 / 1000 | loss 2.1937\n",
            "step  447 / 1000 | loss 1.9914\n",
            "step  448 / 1000 | loss 2.1219\n",
            "step  449 / 1000 | loss 1.7571\n",
            "step  450 / 1000 | loss 2.2230\n",
            "step  451 / 1000 | loss 2.3724\n",
            "step  452 / 1000 | loss 2.6693\n",
            "step  453 / 1000 | loss 2.5797\n",
            "step  454 / 1000 | loss 2.1119\n",
            "step  455 / 1000 | loss 2.4786\n",
            "step  456 / 1000 | loss 1.9844\n",
            "step  457 / 1000 | loss 1.7433\n",
            "step  458 / 1000 | loss 2.5522\n",
            "step  459 / 1000 | loss 1.8889\n",
            "step  460 / 1000 | loss 2.1340\n",
            "step  461 / 1000 | loss 2.3087\n",
            "step  462 / 1000 | loss 2.0797\n",
            "step  463 / 1000 | loss 2.4638\n",
            "step  464 / 1000 | loss 2.1735\n",
            "step  465 / 1000 | loss 1.9122\n",
            "step  466 / 1000 | loss 1.8003\n",
            "step  467 / 1000 | loss 2.0400\n",
            "step  468 / 1000 | loss 1.9730\n",
            "step  469 / 1000 | loss 1.8492\n",
            "step  470 / 1000 | loss 1.9830\n",
            "step  471 / 1000 | loss 2.1642\n",
            "step  472 / 1000 | loss 1.9281\n",
            "step  473 / 1000 | loss 1.5411\n",
            "step  474 / 1000 | loss 1.6225\n",
            "step  475 / 1000 | loss 2.6485\n",
            "step  476 / 1000 | loss 2.5735\n",
            "step  477 / 1000 | loss 2.0460\n",
            "step  478 / 1000 | loss 2.3686\n",
            "step  479 / 1000 | loss 1.4599\n",
            "step  480 / 1000 | loss 1.8700\n",
            "step  481 / 1000 | loss 1.8865\n",
            "step  482 / 1000 | loss 1.6663\n",
            "step  483 / 1000 | loss 1.8585\n",
            "step  484 / 1000 | loss 1.5969\n",
            "step  485 / 1000 | loss 1.7211\n",
            "step  486 / 1000 | loss 2.4041\n",
            "step  487 / 1000 | loss 1.5356\n",
            "step  488 / 1000 | loss 1.9492\n",
            "step  489 / 1000 | loss 2.3788\n",
            "step  490 / 1000 | loss 2.0531\n",
            "step  491 / 1000 | loss 1.8988\n",
            "step  492 / 1000 | loss 1.9725\n",
            "step  493 / 1000 | loss 2.0371\n",
            "step  494 / 1000 | loss 2.1547\n",
            "step  495 / 1000 | loss 2.1279\n",
            "step  496 / 1000 | loss 2.2476\n",
            "step  497 / 1000 | loss 1.4205\n",
            "step  498 / 1000 | loss 2.0307\n",
            "step  499 / 1000 | loss 1.9843\n",
            "step  500 / 1000 | loss 1.4072\n",
            "step  501 / 1000 | loss 2.2121\n",
            "step  502 / 1000 | loss 1.3104\n",
            "step  503 / 1000 | loss 2.1568\n",
            "step  504 / 1000 | loss 1.5909\n",
            "step  505 / 1000 | loss 2.0516\n",
            "step  506 / 1000 | loss 2.5167\n",
            "step  507 / 1000 | loss 1.6724\n",
            "step  508 / 1000 | loss 1.5976\n",
            "step  509 / 1000 | loss 1.2732\n",
            "step  510 / 1000 | loss 1.9151\n",
            "step  511 / 1000 | loss 2.0595\n",
            "step  512 / 1000 | loss 2.2288\n",
            "step  513 / 1000 | loss 2.0172\n",
            "step  514 / 1000 | loss 1.6152\n",
            "step  515 / 1000 | loss 2.7632\n",
            "step  516 / 1000 | loss 2.0622\n",
            "step  517 / 1000 | loss 1.6003\n",
            "step  518 / 1000 | loss 1.3706\n",
            "step  519 / 1000 | loss 2.1244\n",
            "step  520 / 1000 | loss 2.3483\n",
            "step  521 / 1000 | loss 2.1987\n",
            "step  522 / 1000 | loss 1.9363\n",
            "step  523 / 1000 | loss 1.8318\n",
            "step  524 / 1000 | loss 1.8452\n",
            "step  525 / 1000 | loss 2.4538\n",
            "step  526 / 1000 | loss 1.3235\n",
            "step  527 / 1000 | loss 1.7349\n",
            "step  528 / 1000 | loss 2.4666\n",
            "step  529 / 1000 | loss 1.8108\n",
            "step  530 / 1000 | loss 2.2050\n",
            "step  531 / 1000 | loss 1.9774\n",
            "step  532 / 1000 | loss 1.4140\n",
            "step  533 / 1000 | loss 1.3430\n",
            "step  534 / 1000 | loss 2.2312\n",
            "step  535 / 1000 | loss 1.9365\n",
            "step  536 / 1000 | loss 1.7581\n",
            "step  537 / 1000 | loss 1.3414\n",
            "step  538 / 1000 | loss 1.7424\n",
            "step  539 / 1000 | loss 2.8262\n",
            "step  540 / 1000 | loss 1.8464\n",
            "step  541 / 1000 | loss 2.2075\n",
            "step  542 / 1000 | loss 2.0722\n",
            "step  543 / 1000 | loss 1.6364\n",
            "step  544 / 1000 | loss 2.4831\n",
            "step  545 / 1000 | loss 2.1234\n",
            "step  546 / 1000 | loss 2.1432\n",
            "step  547 / 1000 | loss 2.0471\n",
            "step  548 / 1000 | loss 2.3382\n",
            "step  549 / 1000 | loss 2.1629\n",
            "step  550 / 1000 | loss 2.0184\n",
            "step  551 / 1000 | loss 1.7065\n",
            "step  552 / 1000 | loss 1.5358\n",
            "step  553 / 1000 | loss 1.4070\n",
            "step  554 / 1000 | loss 2.1411\n",
            "step  555 / 1000 | loss 2.4160\n",
            "step  556 / 1000 | loss 2.1994\n",
            "step  557 / 1000 | loss 1.8524\n",
            "step  558 / 1000 | loss 2.0903\n",
            "step  559 / 1000 | loss 2.1093\n",
            "step  560 / 1000 | loss 1.4888\n",
            "step  561 / 1000 | loss 1.9293\n",
            "step  562 / 1000 | loss 2.5484\n",
            "step  563 / 1000 | loss 2.0845\n",
            "step  564 / 1000 | loss 2.1544\n",
            "step  565 / 1000 | loss 2.4623\n",
            "step  566 / 1000 | loss 1.8599\n",
            "step  567 / 1000 | loss 1.5504\n",
            "step  568 / 1000 | loss 2.4959\n",
            "step  569 / 1000 | loss 1.8603\n",
            "step  570 / 1000 | loss 2.4185\n",
            "step  571 / 1000 | loss 2.6322\n",
            "step  572 / 1000 | loss 1.9050\n",
            "step  573 / 1000 | loss 1.9713\n",
            "step  574 / 1000 | loss 1.9734\n",
            "step  575 / 1000 | loss 1.7953\n",
            "step  576 / 1000 | loss 2.1058\n",
            "step  577 / 1000 | loss 2.3215\n",
            "step  578 / 1000 | loss 1.9812\n",
            "step  579 / 1000 | loss 1.6520\n",
            "step  580 / 1000 | loss 2.3747\n",
            "step  581 / 1000 | loss 1.4477\n",
            "step  582 / 1000 | loss 2.2735\n",
            "step  583 / 1000 | loss 1.4109\n",
            "step  584 / 1000 | loss 1.2412\n",
            "step  585 / 1000 | loss 1.7655\n",
            "step  586 / 1000 | loss 1.7279\n",
            "step  587 / 1000 | loss 1.4856\n",
            "step  588 / 1000 | loss 2.0041\n",
            "step  589 / 1000 | loss 1.7535\n",
            "step  590 / 1000 | loss 1.8749\n",
            "step  591 / 1000 | loss 1.6976\n",
            "step  592 / 1000 | loss 2.1337\n",
            "step  593 / 1000 | loss 2.4015\n",
            "step  594 / 1000 | loss 1.9023\n",
            "step  595 / 1000 | loss 1.7285\n",
            "step  596 / 1000 | loss 1.3875\n",
            "step  597 / 1000 | loss 2.2287\n",
            "step  598 / 1000 | loss 2.0170\n",
            "step  599 / 1000 | loss 2.0482\n",
            "step  600 / 1000 | loss 2.4541\n",
            "step  601 / 1000 | loss 2.1677\n",
            "step  602 / 1000 | loss 2.2556\n",
            "step  603 / 1000 | loss 1.8935\n",
            "step  604 / 1000 | loss 2.1363\n",
            "step  605 / 1000 | loss 1.6147\n",
            "step  606 / 1000 | loss 1.3713\n",
            "step  607 / 1000 | loss 2.0815\n",
            "step  608 / 1000 | loss 1.9043\n",
            "step  609 / 1000 | loss 1.7543\n",
            "step  610 / 1000 | loss 1.5428\n",
            "step  611 / 1000 | loss 2.0064\n",
            "step  612 / 1000 | loss 2.0220\n",
            "step  613 / 1000 | loss 1.3140\n",
            "step  614 / 1000 | loss 1.8630\n",
            "step  615 / 1000 | loss 1.2333\n",
            "step  616 / 1000 | loss 1.4263\n",
            "step  617 / 1000 | loss 1.7779\n",
            "step  618 / 1000 | loss 2.3281\n",
            "step  619 / 1000 | loss 1.9118\n",
            "step  620 / 1000 | loss 1.9743\n",
            "step  621 / 1000 | loss 2.1240\n",
            "step  622 / 1000 | loss 1.2140\n",
            "step  623 / 1000 | loss 1.7951\n",
            "step  624 / 1000 | loss 1.8300\n",
            "step  625 / 1000 | loss 1.4302\n",
            "step  626 / 1000 | loss 1.9877\n",
            "step  627 / 1000 | loss 2.7454\n",
            "step  628 / 1000 | loss 2.1135\n",
            "step  629 / 1000 | loss 1.7736\n",
            "step  630 / 1000 | loss 1.5325\n",
            "step  631 / 1000 | loss 2.1741\n",
            "step  632 / 1000 | loss 1.7424\n",
            "step  633 / 1000 | loss 2.1341\n",
            "step  634 / 1000 | loss 1.9284\n",
            "step  635 / 1000 | loss 2.0313\n",
            "step  636 / 1000 | loss 1.6349\n",
            "step  637 / 1000 | loss 1.2310\n",
            "step  638 / 1000 | loss 1.7873\n",
            "step  639 / 1000 | loss 1.6998\n",
            "step  640 / 1000 | loss 1.7144\n",
            "step  641 / 1000 | loss 1.9697\n",
            "step  642 / 1000 | loss 1.3800\n",
            "step  643 / 1000 | loss 2.0023\n",
            "step  644 / 1000 | loss 2.2900\n",
            "step  645 / 1000 | loss 2.2131\n",
            "step  646 / 1000 | loss 1.5909\n",
            "step  647 / 1000 | loss 1.8915\n",
            "step  648 / 1000 | loss 1.8354\n",
            "step  649 / 1000 | loss 1.8372\n",
            "step  650 / 1000 | loss 2.4058\n",
            "step  651 / 1000 | loss 1.3376\n",
            "step  652 / 1000 | loss 1.7761\n",
            "step  653 / 1000 | loss 1.4137\n",
            "step  654 / 1000 | loss 1.9794\n",
            "step  655 / 1000 | loss 1.2673\n",
            "step  656 / 1000 | loss 1.5300\n",
            "step  657 / 1000 | loss 1.9617\n",
            "step  658 / 1000 | loss 2.0898\n",
            "step  659 / 1000 | loss 1.3773\n",
            "step  660 / 1000 | loss 1.7858\n",
            "step  661 / 1000 | loss 1.7887\n",
            "step  662 / 1000 | loss 1.9312\n",
            "step  663 / 1000 | loss 1.8747\n",
            "step  664 / 1000 | loss 1.4279\n",
            "step  665 / 1000 | loss 2.1749\n",
            "step  666 / 1000 | loss 1.8770\n",
            "step  667 / 1000 | loss 1.6191\n",
            "step  668 / 1000 | loss 1.9161\n",
            "step  669 / 1000 | loss 1.3845\n",
            "step  670 / 1000 | loss 1.9171\n",
            "step  671 / 1000 | loss 1.2853\n",
            "step  672 / 1000 | loss 1.3082\n",
            "step  673 / 1000 | loss 1.9872\n",
            "step  674 / 1000 | loss 1.8863\n",
            "step  675 / 1000 | loss 2.3106\n",
            "step  676 / 1000 | loss 2.0154\n",
            "step  677 / 1000 | loss 1.8983\n",
            "step  678 / 1000 | loss 1.3749\n",
            "step  679 / 1000 | loss 2.1539\n",
            "step  680 / 1000 | loss 1.3864\n",
            "step  681 / 1000 | loss 2.1327\n",
            "step  682 / 1000 | loss 1.9508\n",
            "step  683 / 1000 | loss 2.2980\n",
            "step  684 / 1000 | loss 2.0408\n",
            "step  685 / 1000 | loss 1.7791\n",
            "step  686 / 1000 | loss 2.1923\n",
            "step  687 / 1000 | loss 2.0609\n",
            "step  688 / 1000 | loss 2.1164\n",
            "step  689 / 1000 | loss 1.1298\n",
            "step  690 / 1000 | loss 1.7167\n",
            "step  691 / 1000 | loss 1.5540\n",
            "step  692 / 1000 | loss 2.1172\n",
            "step  693 / 1000 | loss 1.9908\n",
            "step  694 / 1000 | loss 1.6185\n",
            "step  695 / 1000 | loss 2.3724\n",
            "step  696 / 1000 | loss 1.1079\n",
            "step  697 / 1000 | loss 2.1057\n",
            "step  698 / 1000 | loss 2.0809\n",
            "step  699 / 1000 | loss 2.2907\n",
            "step  700 / 1000 | loss 2.3716\n",
            "step  701 / 1000 | loss 1.4175\n",
            "step  702 / 1000 | loss 1.9796\n",
            "step  703 / 1000 | loss 1.5939\n",
            "step  704 / 1000 | loss 1.7760\n",
            "step  705 / 1000 | loss 2.4322\n",
            "step  706 / 1000 | loss 1.9595\n",
            "step  707 / 1000 | loss 1.9650\n",
            "step  708 / 1000 | loss 2.1999\n",
            "step  709 / 1000 | loss 1.5201\n",
            "step  710 / 1000 | loss 1.0021\n",
            "step  711 / 1000 | loss 1.9341\n",
            "step  712 / 1000 | loss 1.8626\n",
            "step  713 / 1000 | loss 1.9794\n",
            "step  714 / 1000 | loss 1.4165\n",
            "step  715 / 1000 | loss 2.0821\n",
            "step  716 / 1000 | loss 1.7736\n",
            "step  717 / 1000 | loss 1.4098\n",
            "step  718 / 1000 | loss 1.4347\n",
            "step  719 / 1000 | loss 1.5794\n",
            "step  720 / 1000 | loss 2.1044\n",
            "step  721 / 1000 | loss 1.7899\n",
            "step  722 / 1000 | loss 2.0352\n",
            "step  723 / 1000 | loss 1.1701\n",
            "step  724 / 1000 | loss 1.8987\n",
            "step  725 / 1000 | loss 1.9724\n",
            "step  726 / 1000 | loss 2.0051\n",
            "step  727 / 1000 | loss 1.8434\n",
            "step  728 / 1000 | loss 1.9475\n",
            "step  729 / 1000 | loss 1.4525\n",
            "step  730 / 1000 | loss 1.9249\n",
            "step  731 / 1000 | loss 1.4008\n",
            "step  732 / 1000 | loss 1.2788\n",
            "step  733 / 1000 | loss 1.8107\n",
            "step  734 / 1000 | loss 1.9956\n",
            "step  735 / 1000 | loss 1.8894\n",
            "step  736 / 1000 | loss 1.7907\n",
            "step  737 / 1000 | loss 1.3543\n",
            "step  738 / 1000 | loss 2.0556\n",
            "step  739 / 1000 | loss 2.0429\n",
            "step  740 / 1000 | loss 1.6176\n",
            "step  741 / 1000 | loss 1.1141\n",
            "step  742 / 1000 | loss 1.9286\n",
            "step  743 / 1000 | loss 1.1048\n",
            "step  744 / 1000 | loss 1.9319\n",
            "step  745 / 1000 | loss 1.6578\n",
            "step  746 / 1000 | loss 1.6781\n",
            "step  747 / 1000 | loss 2.0471\n",
            "step  748 / 1000 | loss 2.3068\n",
            "step  749 / 1000 | loss 1.6811\n",
            "step  750 / 1000 | loss 1.5814\n",
            "step  751 / 1000 | loss 1.6646\n",
            "step  752 / 1000 | loss 1.2534\n",
            "step  753 / 1000 | loss 1.8717\n",
            "step  754 / 1000 | loss 1.8969\n",
            "step  755 / 1000 | loss 1.6289\n",
            "step  756 / 1000 | loss 2.2201\n",
            "step  757 / 1000 | loss 1.6553\n",
            "step  758 / 1000 | loss 1.0781\n",
            "step  759 / 1000 | loss 1.0625\n",
            "step  760 / 1000 | loss 1.9706\n",
            "step  761 / 1000 | loss 1.5381\n",
            "step  762 / 1000 | loss 1.5988\n",
            "step  763 / 1000 | loss 1.8142\n",
            "step  764 / 1000 | loss 2.0822\n",
            "step  765 / 1000 | loss 1.0061\n",
            "step  766 / 1000 | loss 1.6935\n",
            "step  767 / 1000 | loss 1.8575\n",
            "step  768 / 1000 | loss 1.5778\n",
            "step  769 / 1000 | loss 2.2347\n",
            "step  770 / 1000 | loss 1.4790\n",
            "step  771 / 1000 | loss 1.1929\n",
            "step  772 / 1000 | loss 2.1382\n",
            "step  773 / 1000 | loss 1.9148\n",
            "step  774 / 1000 | loss 1.9572\n",
            "step  775 / 1000 | loss 1.7316\n",
            "step  776 / 1000 | loss 1.5042\n",
            "step  777 / 1000 | loss 2.1158\n",
            "step  778 / 1000 | loss 1.9403\n",
            "step  779 / 1000 | loss 1.8723\n",
            "step  780 / 1000 | loss 1.3602\n",
            "step  781 / 1000 | loss 1.8069\n",
            "step  782 / 1000 | loss 1.9425\n",
            "step  783 / 1000 | loss 1.6451\n",
            "step  784 / 1000 | loss 1.7667\n",
            "step  785 / 1000 | loss 1.7274\n",
            "step  786 / 1000 | loss 1.9637\n",
            "step  787 / 1000 | loss 1.5024\n",
            "step  788 / 1000 | loss 1.9417\n",
            "step  789 / 1000 | loss 1.2881\n",
            "step  790 / 1000 | loss 2.0641\n",
            "step  791 / 1000 | loss 1.2739\n",
            "step  792 / 1000 | loss 1.0902\n",
            "step  793 / 1000 | loss 2.0012\n",
            "step  794 / 1000 | loss 1.6658\n",
            "step  795 / 1000 | loss 1.5057\n",
            "step  796 / 1000 | loss 1.5850\n",
            "step  797 / 1000 | loss 1.9368\n",
            "step  798 / 1000 | loss 2.0846\n",
            "step  799 / 1000 | loss 1.5169\n",
            "step  800 / 1000 | loss 1.5572\n",
            "step  801 / 1000 | loss 1.8885\n",
            "step  802 / 1000 | loss 1.1344\n",
            "step  803 / 1000 | loss 1.5340\n",
            "step  804 / 1000 | loss 1.7515\n",
            "step  805 / 1000 | loss 1.3352\n",
            "step  806 / 1000 | loss 1.6837\n",
            "step  807 / 1000 | loss 1.6852\n",
            "step  808 / 1000 | loss 2.3818\n",
            "step  809 / 1000 | loss 1.1869\n",
            "step  810 / 1000 | loss 1.1875\n",
            "step  811 / 1000 | loss 1.4875\n",
            "step  812 / 1000 | loss 1.6508\n",
            "step  813 / 1000 | loss 2.2283\n",
            "step  814 / 1000 | loss 1.6445\n",
            "step  815 / 1000 | loss 1.9805\n",
            "step  816 / 1000 | loss 2.2816\n",
            "step  817 / 1000 | loss 1.8163\n",
            "step  818 / 1000 | loss 1.7465\n",
            "step  819 / 1000 | loss 1.4396\n",
            "step  820 / 1000 | loss 1.1380\n",
            "step  821 / 1000 | loss 1.2103\n",
            "step  822 / 1000 | loss 1.7186\n",
            "step  823 / 1000 | loss 1.8275\n",
            "step  824 / 1000 | loss 1.2360\n",
            "step  825 / 1000 | loss 1.4851\n",
            "step  826 / 1000 | loss 1.5165\n",
            "step  827 / 1000 | loss 1.8336\n",
            "step  828 / 1000 | loss 1.5680\n",
            "step  829 / 1000 | loss 1.7910\n",
            "step  830 / 1000 | loss 2.2986\n",
            "step  831 / 1000 | loss 1.8525\n",
            "step  832 / 1000 | loss 2.0257\n",
            "step  833 / 1000 | loss 1.9635\n",
            "step  834 / 1000 | loss 0.9973\n",
            "step  835 / 1000 | loss 1.7850\n",
            "step  836 / 1000 | loss 2.0802\n",
            "step  837 / 1000 | loss 1.9420\n",
            "step  838 / 1000 | loss 2.1857\n",
            "step  839 / 1000 | loss 1.9189\n",
            "step  840 / 1000 | loss 1.7898\n",
            "step  841 / 1000 | loss 1.7456\n",
            "step  842 / 1000 | loss 1.8830\n",
            "step  843 / 1000 | loss 1.8609\n",
            "step  844 / 1000 | loss 1.3312\n",
            "step  845 / 1000 | loss 2.1083\n",
            "step  846 / 1000 | loss 1.2994\n",
            "step  847 / 1000 | loss 1.7889\n",
            "step  848 / 1000 | loss 1.7375\n",
            "step  849 / 1000 | loss 1.8066\n",
            "step  850 / 1000 | loss 1.7219\n",
            "step  851 / 1000 | loss 1.9501\n",
            "step  852 / 1000 | loss 2.0144\n",
            "step  853 / 1000 | loss 1.6482\n",
            "step  854 / 1000 | loss 1.1290\n",
            "step  855 / 1000 | loss 2.0658\n",
            "step  856 / 1000 | loss 1.9907\n",
            "step  857 / 1000 | loss 1.2265\n",
            "step  858 / 1000 | loss 1.2873\n",
            "step  859 / 1000 | loss 0.9922\n",
            "step  860 / 1000 | loss 1.1734\n",
            "step  861 / 1000 | loss 2.4256\n",
            "step  862 / 1000 | loss 1.5022\n",
            "step  863 / 1000 | loss 1.0489\n",
            "step  864 / 1000 | loss 1.4207\n",
            "step  865 / 1000 | loss 1.9699\n",
            "step  866 / 1000 | loss 2.4655\n",
            "step  867 / 1000 | loss 1.7272\n",
            "step  868 / 1000 | loss 2.1092\n",
            "step  869 / 1000 | loss 1.7657\n",
            "step  870 / 1000 | loss 2.0691\n",
            "step  871 / 1000 | loss 0.9166\n",
            "step  872 / 1000 | loss 1.6493\n",
            "step  873 / 1000 | loss 1.6774\n",
            "step  874 / 1000 | loss 1.6766\n",
            "step  875 / 1000 | loss 1.4051\n",
            "step  876 / 1000 | loss 1.0687\n",
            "step  877 / 1000 | loss 1.8858\n",
            "step  878 / 1000 | loss 1.6409\n",
            "step  879 / 1000 | loss 1.5162\n",
            "step  880 / 1000 | loss 1.8658\n",
            "step  881 / 1000 | loss 1.7427\n",
            "step  882 / 1000 | loss 1.6976\n",
            "step  883 / 1000 | loss 1.2740\n",
            "step  884 / 1000 | loss 1.2231\n",
            "step  885 / 1000 | loss 1.7604\n",
            "step  886 / 1000 | loss 1.0370\n",
            "step  887 / 1000 | loss 1.4178\n",
            "step  888 / 1000 | loss 1.9148\n",
            "step  889 / 1000 | loss 1.3743\n",
            "step  890 / 1000 | loss 2.1328\n",
            "step  891 / 1000 | loss 1.5751\n",
            "step  892 / 1000 | loss 2.0873\n",
            "step  893 / 1000 | loss 1.8781\n",
            "step  894 / 1000 | loss 1.2482\n",
            "step  895 / 1000 | loss 1.5691\n",
            "step  896 / 1000 | loss 1.2572\n",
            "step  897 / 1000 | loss 1.1307\n",
            "step  898 / 1000 | loss 2.2215\n",
            "step  899 / 1000 | loss 1.7710\n",
            "step  900 / 1000 | loss 1.8913\n",
            "step  901 / 1000 | loss 1.3481\n",
            "step  902 / 1000 | loss 1.4778\n",
            "step  903 / 1000 | loss 1.8115\n",
            "step  904 / 1000 | loss 1.5587\n",
            "step  905 / 1000 | loss 1.1488\n",
            "step  906 / 1000 | loss 1.6403\n",
            "step  907 / 1000 | loss 1.6897\n",
            "step  908 / 1000 | loss 1.8463\n",
            "step  909 / 1000 | loss 1.5732\n",
            "step  910 / 1000 | loss 1.8228\n",
            "step  911 / 1000 | loss 1.2269\n",
            "step  912 / 1000 | loss 1.8669\n",
            "step  913 / 1000 | loss 1.9796\n",
            "step  914 / 1000 | loss 1.5005\n",
            "step  915 / 1000 | loss 1.3261\n",
            "step  916 / 1000 | loss 1.3983\n",
            "step  917 / 1000 | loss 1.3073\n",
            "step  918 / 1000 | loss 1.3962\n",
            "step  919 / 1000 | loss 1.9093\n",
            "step  920 / 1000 | loss 1.0916\n",
            "step  921 / 1000 | loss 1.6372\n",
            "step  922 / 1000 | loss 1.6031\n",
            "step  923 / 1000 | loss 1.2898\n",
            "step  924 / 1000 | loss 1.2871\n",
            "step  925 / 1000 | loss 1.8021\n",
            "step  926 / 1000 | loss 1.3660\n",
            "step  927 / 1000 | loss 0.9758\n",
            "step  928 / 1000 | loss 1.1438\n",
            "step  929 / 1000 | loss 1.0200\n",
            "step  930 / 1000 | loss 1.6250\n",
            "step  931 / 1000 | loss 1.8345\n",
            "step  932 / 1000 | loss 1.7781\n",
            "step  933 / 1000 | loss 1.7180\n",
            "step  934 / 1000 | loss 1.4351\n",
            "step  935 / 1000 | loss 1.3807\n",
            "step  936 / 1000 | loss 1.4682\n",
            "step  937 / 1000 | loss 1.7067\n",
            "step  938 / 1000 | loss 1.9567\n",
            "step  939 / 1000 | loss 1.5862\n",
            "step  940 / 1000 | loss 1.2939\n",
            "step  941 / 1000 | loss 1.2148\n",
            "step  942 / 1000 | loss 1.4264\n",
            "step  943 / 1000 | loss 1.9807\n",
            "step  944 / 1000 | loss 1.3810\n",
            "step  945 / 1000 | loss 1.6181\n",
            "step  946 / 1000 | loss 1.7200\n",
            "step  947 / 1000 | loss 1.7503\n",
            "step  948 / 1000 | loss 2.1859\n",
            "step  949 / 1000 | loss 1.9775\n",
            "step  950 / 1000 | loss 1.1808\n",
            "step  951 / 1000 | loss 1.4686\n",
            "step  952 / 1000 | loss 1.9557\n",
            "step  953 / 1000 | loss 1.6908\n",
            "step  954 / 1000 | loss 1.8442\n",
            "step  955 / 1000 | loss 1.6317\n",
            "step  956 / 1000 | loss 1.0942\n",
            "step  957 / 1000 | loss 1.2987\n",
            "step  958 / 1000 | loss 1.6024\n",
            "step  959 / 1000 | loss 1.3384\n",
            "step  960 / 1000 | loss 1.6248\n",
            "step  961 / 1000 | loss 1.6574\n",
            "step  962 / 1000 | loss 1.5700\n",
            "step  963 / 1000 | loss 2.3128\n",
            "step  964 / 1000 | loss 1.4186\n",
            "step  965 / 1000 | loss 1.4513\n",
            "step  966 / 1000 | loss 1.6077\n",
            "step  967 / 1000 | loss 1.3554\n",
            "step  968 / 1000 | loss 1.6766\n",
            "step  969 / 1000 | loss 1.6569\n",
            "step  970 / 1000 | loss 1.8736\n",
            "step  971 / 1000 | loss 1.6376\n",
            "step  972 / 1000 | loss 1.8565\n",
            "step  973 / 1000 | loss 1.1921\n",
            "step  974 / 1000 | loss 2.2799\n",
            "step  975 / 1000 | loss 2.3341\n",
            "step  976 / 1000 | loss 2.0924\n",
            "step  977 / 1000 | loss 1.1869\n",
            "step  978 / 1000 | loss 1.6680\n",
            "step  979 / 1000 | loss 2.1501\n",
            "step  980 / 1000 | loss 1.3671\n",
            "step  981 / 1000 | loss 1.8837\n",
            "step  982 / 1000 | loss 0.9553\n",
            "step  983 / 1000 | loss 1.2271\n",
            "step  984 / 1000 | loss 1.9697\n",
            "step  985 / 1000 | loss 1.9692\n",
            "step  986 / 1000 | loss 1.2782\n",
            "step  987 / 1000 | loss 2.0686\n",
            "step  988 / 1000 | loss 1.3940\n",
            "step  989 / 1000 | loss 1.3538\n",
            "step  990 / 1000 | loss 1.9937\n",
            "step  991 / 1000 | loss 0.9666\n",
            "step  992 / 1000 | loss 1.4983\n",
            "step  993 / 1000 | loss 1.5489\n",
            "step  994 / 1000 | loss 1.0323\n",
            "step  995 / 1000 | loss 1.8825\n",
            "step  996 / 1000 | loss 1.5761\n",
            "step  997 / 1000 | loss 0.9299\n",
            "step  998 / 1000 | loss 2.2349\n",
            "step  999 / 1000 | loss 1.9336\n",
            "step 1000 / 1000 | loss 1.8137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_log_with_gpt(log_text):\n",
        "    # Maximum length for the longest label we expect to generate (e.g., \"WARNING\" is 7 chars).\n",
        "    max_label_length = 7\n",
        "    # The suffix we append to the log text\n",
        "    prompt_suffix = \" -> \"\n",
        "\n",
        "    # Calculate the maximum allowed length for the log_text part of the prompt\n",
        "    # This ensures that (truncated_log_text + prompt_suffix + generated_label) fits within block_size.\n",
        "    max_effective_prompt_length = block_size - max_label_length\n",
        "    max_log_text_length = max_effective_prompt_length - len(prompt_suffix)\n",
        "\n",
        "    # Ensure max_log_text_length is not negative in case block_size is very small\n",
        "    if max_log_text_length < 0:\n",
        "        max_log_text_length = 0\n",
        "\n",
        "    # Truncate the log_text if it's too long to leave space for generation\n",
        "    processed_log_text = log_text.strip()\n",
        "    if len(processed_log_text) > max_log_text_length:\n",
        "        processed_log_text = processed_log_text[:max_log_text_length]\n",
        "\n",
        "    prompt = processed_log_text + prompt_suffix\n",
        "\n",
        "    # Reset KV cache\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "\n",
        "    # Prime the model with the prepared prompt\n",
        "    priming_token_ids = []\n",
        "    for ch in prompt:\n",
        "        if ch in uchars:\n",
        "            priming_token_ids.append(uchars.index(ch))\n",
        "        # Unknown characters are skipped. Consider adding a special UNK token if needed.\n",
        "\n",
        "    # Initialize the token_id for the next gpt call (for generation)\n",
        "    # and the current position in the sequence\n",
        "    current_token_id_for_generation = BOS # Default to BOS if no priming tokens\n",
        "    current_pos_for_generation = 0\n",
        "\n",
        "    if priming_token_ids:\n",
        "        for pos_id_prime, token_id_in_prompt in enumerate(priming_token_ids):\n",
        "            # Feed each token from the prompt to the GPT model\n",
        "            gpt(token_id_in_prompt, pos_id_prime, keys, values)\n",
        "            current_pos_for_generation = pos_id_prime + 1\n",
        "            current_token_id_for_generation = token_id_in_prompt # Last token primed becomes input for first generation step\n",
        "\n",
        "    # Generate label characters\n",
        "    generated = \"\"\n",
        "    # Generation starts from the position immediately after the prompt\n",
        "    # and continues up to the block_size limit.\n",
        "    for pos_id_gen in range(current_pos_for_generation, block_size):\n",
        "        # Get logits for the next token using the last generated/primed token and current position\n",
        "        logits = gpt(current_token_id_for_generation, pos_id_gen, keys, values)\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        # Select the token with the highest probability (argmax) for classification\n",
        "        next_token_id = max(range(vocab_size), key=lambda i: probs[i].data)\n",
        "\n",
        "        # If the model predicts the Beginning of Sequence (BOS) token, it signals end of generation.\n",
        "        if next_token_id == BOS:\n",
        "            break\n",
        "\n",
        "        # Convert token ID back to character and append to generated string\n",
        "        next_char = uchars[next_token_id]\n",
        "        generated += next_char\n",
        "        current_token_id_for_generation = next_token_id # Update for the next generation step\n",
        "\n",
        "\n",
        "    if \"WARNING\" in generated:\n",
        "        return \"WARNING\"\n",
        "\n",
        "    if \"ERROR\" in generated:\n",
        "        return \"ERROR\"\n",
        "    if \"INFO\" in generated:\n",
        "        return \"INFO\"\n",
        "\n",
        "    # If no keyword is found after generating up to block_size, return \"UNCERTAIN\"\n",
        "    return \"UNCERTAIN\""
      ],
      "metadata": {
        "id": "4OrAv0ES8BEX"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classify_log_with_gpt(\"audit-trace cache hit scheduled\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E51z-qxMTFQD",
        "outputId": "3f4ef709-5ebf-4547-fa76-b5bb614d8098"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classify_log_with_gpt(\"Syntax Error\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm77upJSTlru",
        "outputId": "a09adab3-b1c5-4fc6-c09a-65e255282ce3"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classify_log_with_gpt(\"Warning Limit Hit \"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhHLj_C7WYoc",
        "outputId": "9ba71b49-83e1-4643-e957-b995c9c3d9a6"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio UI**"
      ],
      "metadata": {
        "id": "ccHuYew-XecK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "623f4a13",
        "outputId": "1a6e80c8-ff1f-4699-fcd9-9c8fd8679ebd"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def classify_log_interface(log_input):\n",
        "    return classify_log_with_gpt(log_input)\n",
        "\n",
        "custom_css = \"\"\"\n",
        "body { font-family: 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif; background-color: #f0f2f5; }\n",
        ".gradio-container { max-width: 900px; margin: 20px auto; padding: 20px; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); background-color: white; }\n",
        ".gr-button { background-color: #1a73e8; color: white; border-radius: 8px; padding: 10px 20px; font-weight: bold; }\n",
        ".gr-button:hover { background-color: #155cb8; }\n",
        ".gr-textbox, .gr-textarea { border-radius: 8px; border: 1px solid #ccc; padding: 10px; box-shadow: inset 0 1px 3px rgba(0,0,0,0.05); }\n",
        ".gr-label { font-weight: 600; color: #333; margin-bottom: 5px; }\n",
        "h1, h2, h3, h4, h5, h6 { color: #202124; }\n",
        "\"\"\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=classify_log_interface,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter log message here...\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Label\"),\n",
        "    title=\"Log Classifier Micro GPT\",\n",
        "    description=\"Enter a log message to classify it as ERROR, WARNING, INFO, or UNCERTAIN.\",\n",
        "    live=False, # Changed to False for manual submission\n",
        "    allow_flagging=\"never\",\n",
        "    theme=\"soft\", # Using a modern Gradio theme\n",
        "    css=custom_css\n",
        ")\n",
        "\n",
        "# Set the footer after the interface is created\n",
        "iface.footer = \"<p>Created by - Shubham Murtadak (AI/ML Engineer)</p>\"\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://fb493364b2672bc528.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fb493364b2672bc528.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://fb493364b2672bc528.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QhLuwOMhXvLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}